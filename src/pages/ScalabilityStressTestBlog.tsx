import '../styles/SlowApiRescueBlog.css'; // Reusing the blog styles
import { Link } from 'react-router-dom';

const ScalabilityStressTestBlog = () => {
    return (
        <div className="blog-post-page">
            <Link to="/practice-projects" className="blog-back-link">‚Üê Back to Practice Projects</Link>

            <h1>üìâ Scalability Stress Test: Surviving the 10k User Surge</h1>

            <p>
                It's launch day. Marketing sent a push notification. 10,000 users hit your API in 5 minutes. The server crashes.
            </p>

            <p>
                In this project, I simulated this exact scenario. I took a standard backend that died at 200 concurrent requests
                and optimized it to handle <strong>2,000+ simultaneous connections</strong> without breaking a sweat.
            </p>

            <hr style={{ borderColor: '#334155', margin: '2rem 0' }} />

            <h2>‚ùå The Problem</h2>
            <p>The backend worked fine locally, but it was fragile.</p>

            <h3>Key Bottlenecks Identified during Load Testing (k6/JMeter):</h3>
            <ul>
                <li><strong>Connection Exhaustion</strong>: The API opened a new DB connection for every request, hitting the database limit (max 100 connections).</li>
                <li><strong>CPU Spikes</strong>: Heavy computation (e.g., image processing) blocked the main Event Loop.</li>
                <li><strong>No Rate Limiting</strong>: A single script could DoS connection slots.</li>
                <li><strong>Redundant Reads</strong>: Static data (like "Config Settings") was fetched from DB on every request.</li>
            </ul>

            <h2>üéØ The Goal</h2>
            <p><strong>Objective</strong>: Make the backend scalable and production-ready for high traffic.</p>
            <p><strong>Success Criteria</strong>:</p>
            <ul>
                <li>Handle 1k+ concurrent users with &lt;500ms latency.</li>
                <li>Zero downtime during load spikes.</li>
                <li>Client-facing outcome: <em>"Made backend scalable, handling 10x traffic surge without crashing."</em></li>
            </ul>

            <h2>‚úÖ Technical Implementation & Fixes</h2>

            <h3>1. Database Connection Pooling</h3>
            <p><strong>The "Bad" Approach:</strong></p>
            <p>Connecting inside the request handler.</p>

            <pre><code>{`// ‚ùå BAD: New connection per request
app.get('/users', async (req, res) => {
  const client = new Client(); // Opens TCP socket
  await client.connect();
  const result = await client.query('SELECT * FROM users');
  await client.end();
});`}</code></pre>

            <p><strong>The "Good" Approach (Pooling):</strong></p>
            <p>I implemented a <strong>Connection Pool</strong> that maintains 20 alive connections and reuses them.</p>
            <pre><code>{`// ‚úÖ GOOD: Pool reuse
const pool = new Pool({ max: 20 });
app.get('/users', async (req, res) => {
  const result = await pool.query('SELECT * FROM users'); // Reuses existing connection
});`}</code></pre>

            <h3>2. Caching Strategy (Redis)</h3>
            <p>70% of database reads were for the same 5 settings.</p>

            <p><strong>The Fix:</strong></p>
            <p>I implemented <strong>Redis Caching</strong> (Write-Through pattern).</p>

            <pre><code>{`// ‚úÖ GOOD: Cache-First Strategy
async function getConfig() {
  const cache = await redis.get('site_config');
  if (cache) return JSON.parse(cache);

  const data = await db.find('config');
  await redis.set('site_config', JSON.stringify(data), 'EX', 3600);
  return data;
}`}</code></pre>
            <p><strong>Result</strong>: DB load dropped by <strong>60%</strong>.</p>

            <h3>3. Rate Limiting (Throttling)</h3>
            <p>To prevent abuse, I added loose and strict rate limits using <code>redis</code>-backed middleware.</p>

            <pre><code>{`// ‚úÖ GOOD: Protection
app.use(rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // limit each IP to 100 requests per window
  message: "Too many requests, please try again later."
}));`}</code></pre>

            <h3>4. Horizontal Scaling (Clustering)</h3>
            <p>Node.js is single-threaded. By default, it uses only 1 CPU core, ignoring the other 7 on the server.</p>
            <p><strong>The Fix:</strong></p>
            <p>I used the <strong>Cluster Module</strong> (or pm2) to fork connected worker processes.</p>

            <pre><code>{`// ‚úÖ GOOD: Utilizing all Cores
if (cluster.isMaster) {
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }
} else {
  app.listen(3000);
}`}</code></pre>
            <p><strong>Result</strong>: Throughput increased <strong>4x</strong> (on a 4-core machine).</p>

            <h2>üß™ Results & Metrics</h2>

            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Before Optimization</th>
                        <th>After Optimization</th>
                        <th>Improvement</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Max Concurrent Users</strong></td>
                        <td>180 (Crashed)</td>
                        <td>2,500 (Stable)</td>
                        <td><span className="blog-metric-highlight">13x Capacity</span></td>
                    </tr>
                    <tr>
                        <td><strong>Avg Latency (Load)</strong></td>
                        <td>TIMEOUT</td>
                        <td>320 ms</td>
                        <td><span className="blog-metric-highlight">Usable</span></td>
                    </tr>
                    <tr>
                        <td><strong>CPU Usage</strong></td>
                        <td>100% (Single Core)</td>
                        <td>80% (All Cores)</td>
                        <td><span className="blog-metric-highlight">Efficient</span></td>
                    </tr>
                </tbody>
            </table>

            <hr style={{ borderColor: '#334155', margin: '3rem 0' }} />

            <h2>üöÄ Conclusion</h2>
            <p>
                Scalability isn't just "buying bigger servers." It's about efficient resource usage. By pooling connections,
                caching hot data, and utilizing multiple cores, I prepared the system to handle the "viral moment" without crashing.
            </p>

            <div style={{ marginTop: '2rem', padding: '1.5rem', background: '#1e293b', borderRadius: '8px' }}>
                <h3 style={{ marginTop: 0 }}>üëâ Ready to Stress Test?</h3>
                <p>Fire up JMeter and optimize the backend in the practice project.</p>
                <Link to="/practice-projects" className="button" style={{
                    display: 'inline-block',
                    background: '#3b82f6',
                    color: 'white',
                    padding: '0.75rem 1.5rem',
                    borderRadius: '6px',
                    textDecoration: 'none',
                    fontWeight: 'bold'
                }}>Go to Projects Board</Link>
            </div>
        </div>
    );
};

export default ScalabilityStressTestBlog;
